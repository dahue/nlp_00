{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/atissera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/atissera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/atissera/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviews about electronic products sentiment analysis \n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download stopwords\n",
    "download('stopwords')\n",
    "download('punkt')\n",
    "download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_map = {}\n",
    "current_index = 0\n",
    "\n",
    "# Custom tokenizer\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "# Construct vectors\n",
    "def tokens_to_vextor(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1)\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum()\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematizer is like a word normalizer, converts words to its base form. \n",
    "# Words like 'dogs' -> 'dog'\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "positive_reviews = BeautifulSoup(open('dataset/electronics/positive.review').read())\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('dataset/electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index +=1\n",
    "        \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vextor(tokens, 1)\n",
    "    data[i, :] = xy\n",
    "    i += 1\n",
    "    \n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vextor(tokens, 0)\n",
    "    data[i, :] = xy\n",
    "    i += 1\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=0, \n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate using Logistic Regression:  0.7525\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "clf.fit(X_train, y_train)\n",
    "print('Classification rate using Logistic Regression: ', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time -0.5144916927408415\n",
      "used 0.660849933673563\n",
      "'ve 0.5680812605907062\n",
      "need 0.5337997860528654\n",
      "good 1.6530157927786526\n",
      "sound 0.769016913090303\n",
      "like 0.5557346109636143\n",
      "n't -1.5479801167539364\n",
      "easy 1.1513953442451175\n",
      "get -0.7130665021709531\n",
      "use 1.4436948166965653\n",
      "quality 1.1443826954207381\n",
      "best 0.8567242424719295\n",
      "item -0.9572426473949455\n",
      "well 0.9595405830784492\n",
      "wa -0.936713512291071\n",
      "perfect 0.9018938027631505\n",
      "fast 0.6145687617755199\n",
      "price 1.7983290325683774\n",
      "great 2.984832193586256\n",
      "money -0.7277022524366462\n",
      "memory 0.6254305223878249\n",
      "would -0.7493284066592155\n",
      "buy -0.8798767529822638\n",
      "worked -0.6723029280919447\n",
      "doe -0.6841910628536952\n",
      "highly 0.7192355008114237\n",
      "recommend 0.5970071914703003\n",
      "first -0.5915455849188291\n",
      "support -0.7195952355178907\n",
      "little 0.6463614654577288\n",
      "returned -0.5908530320915524\n",
      "excellent 1.0242130710782529\n",
      "love 0.8550367450904176\n",
      "mouse 0.5742760634612201\n",
      "thing -0.6913874624088223\n",
      "even -0.5602511574326824\n",
      "poor -0.5239571208700422\n",
      "back -1.3161404494454627\n",
      "speaker 0.587848639838447\n",
      "paper 0.5135498628226807\n",
      "return -0.9055351097571267\n",
      "waste -0.6632050228341781\n"
     ]
    }
   ],
   "source": [
    "# Check the sentiment value for words above the threshold\n",
    "threshold = 0.5\n",
    "for word, index in word_index_map.items():\n",
    "    weight = clf.coef_[0][index]\n",
    "    if abs(weight) > threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
